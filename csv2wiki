#!/usr/bin/env python
#
# Convert a CSV file to wiki pages.  Run 'csv2wiki --help' for details.
# https://github.com/OpenTechStrategies/ots-tools/blob/master/csv2wiki
#
# Copyright (C) 2017 Open Tech Strategies, LLC
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

__doc__ = """\
Convert each row of a UTF-8 or Latin-1 CSV file to a MediaWiki page.

Basic usage:

  $ csv2wiki -c CONFIG_FILE [ --delete-matching=PATTERN ] CSV_FILE

The CONFIG_FILE contains the wiki URL, login information, and various
other run-time parameters.  It is in standard .ini file format, with
a sole "[default]" section and the following elements in that section:

  wiki_url:            The url of the wiki, e.g. "http://localhost/mediawiki",
                       "https://www.example.com/mwiki", etc.

  username:            User account with write/create permission in the wiki.
                       (With --delete, user needs page-delete permission.)

  password:            The wiki password corresponding to the username.

  toc_name:            Title for the generated Table of Contents page.

  title_tmpl:          Template into which selected column values from
                       a row are substituted to create the title of
                       each row's page.  Columns are specified with
                       "{N}" in the string, where N is a column number.
                       The special column {0} is the CSV row number,
                       which is automatically left-padded with zeros
                       appropriately for the total number of rows.

                       For example:

                       Suppose title_tmpl is "Entry_{1}_{3}_{0}",
                       column 1 contains "foo", column 3 has "bar",
                       and this is row 15 of a CSV file with 250 rows.
                       The page title corresponding to this row would
                       be "Entry_foo_bar_015".

                       *NOTE:* 

                       The titles generated this way must be unique.
                       Typically, you ensure uniqueness by making sure
                       that at least one element of title_tmpl is some
                       kind of unique identifying number -- either a
                       column whose value is known to be unique, or
                       the row number.

                       If the generated page titles are not unique,
                       only the last row in the set of duplicates will
                       appear in the wiki.  (TODO: we could solve this
                       by checking for collisions and adding a unique
                       tail when one happens, but we currently don't
                       do that.  So, be careful.)

  cat_col:             The number of the column (if any) in the CSV
                       file that should be used to create a category
                       for that row; column numbering begins at 1, not
                       0.  Omit this, or leave the value blank, to not
                       use categories at all.

  delimiter:           A single-char delimiter used to separate columns
                       in a CSV row.  If omitted, defaults to ','.

  quotechar:           A single-char quotechar used to wrap contents of
                       a single cell in the CSV file.  If omitted, 
                       defaults to '"'.

  path_to_api:         The API path under the URL; defaults to "/"; see
                       mwclient.readthedocs.io/en/master/user/connecting.html.
                       Note that on Wikipedia.org, this would be "/w/", 
                       but in most non-Wikipedia instances of MediaWiki,
                       the default "/" is more likely.  So if you're using
                       that default, you don't have to specify this at all.

Example config file
-------------------

Here is an example config file:

  [default]
  wiki_url: http://localhost/mediawiki
  username: wikibot
  password: bqaRY76gtXu
  title_tmpl: Entry_{1}
  toc_name: List_of_Entries
  cat_col: 5

The "[default]" section name at the top must be present.  The .ini
format always has sections, and for the sake of forward compatibility
this script requires the one section's name to be "default".

Page deletion support
---------------------

This script also supports batch deletion of pages, since a possible
outcome of initial runs is that you have a bunch of pages in your wiki
that turn out to be not ready.  Use the "--delete-matching PATTERN"
option to remove them.  All pages whose names match PATTERN will be 
deleted (and no pages will be created).

Typically, PATTERN will be similar to the title_tmpl config element,
since --delete-matching is most often used to delete pages originally
created with this script.

NOTE: We will probably change --delete to just re-calculate page
      titles based on the CSV input, instead of taking a PATTERN
      argument and then querying the MediaWiki API for a list of pages
      whose names match that PATTERN.  If we make this change, then
      the --delete-matching=PATTERN option will become just --delete
      with no argument.

Dependencies and Troubleshooting
--------------------------------

* This requires the 'mwclient' and 'unidecode' Python libraries

  If you get an error that looks something like this:

    Traceback (most recent call last):
      File "./csv2wiki", line 27, in <module>
        from mwclient import Site
    ImportError: No module named mwclient

  or this:

    Traceback (most recent call last):
      File "./csv2wiki", line 27, in <module>
        from unidecode import unidecode
    ImportError: No module named unidecode

  then run these commands

    $ sudo pip install mwclient
    $ sudo pip install unidecode

  and try again.

* If you run create/delete multiple times, you may need to run
  
    $ php maintenance/rebuildall.php
  
  in your MediaWiki instance to link pages to their categories properly.
  That script takes about 10 minutes to run for a wiki with <300 pages.
  
* If you get errors saving some pages, it may be an anti-spam plugin.
  
  If your MediaWiki instance has Extension:SpamBlacklist enabled,
  then you may get errors when trying to create pages that contain
  certain kinds of URLs or email addresses (namely, URLs or email
  addresses that SpamBlacklist thinks look spammy). 
  
  One solution is to just turn off Extension:SpamBlacklist entirely.
  But even if you don't have that kind of administrative access,
  you might still have enough access to *configure* the extension, 
  in which case you can whitelist everything via a catchall regexp.
  Visit one or of of these pages:
  
    https://mywiki.example.com/index.php?title=MediaWiki:Spam-whitelist
    https://mywiki.example.com/index.php?title=MediaWiki:Email-whitelist
  
  You'll see a commented-out explanation of how the whitelist works.
  Just add a line with the regular expression ".*", as in this example:
  
    # External URLs matching this list will *not* be blocked even if they would
    # have been blocked by blacklist entries.
    #
    # Syntax is as follows:
    #   * Everything from a "#" character to the end of the line is a comment
    #   * Every non-blank line is a regex fragment which will only match hosts inside URLs
    .*
  
  That will let you save a page containing any URL.  (Things work
  similarly on the Email-whitelist page).

* Run time may be slower than you expect.

  Creating 250 wiki pages takes about 5 minutes on localhost.  
  Part of the reason for this is that the script does a page save
  for every section within a page; see the TODO comment near the 
  page.save() calls in the loop over cells for more.  We may look
  into whether the 'mwclient' library offers a caching option, or
  maybe we'll create the whole page and save it once at the end.
"""

import csv
from mwclient import Site
from mwclient import errors
import unidecode
import getopt, sys
import ConfigParser

# For exception matching.
import requests

class WikiSession:
    """One CSV import session with a given user and a given wiki."""
    def __init__(self, csv_file, config_obj):
        """Log in to a wiki session to create pages based on CSV_FILE.
        Take login parameters from CONFIG_OBJ."""
        self.site_conn           = None  # will be a mwclient Site object
        self.csv_input           = None  # will be csv.reader object
        self.row_count           = None  # will be num rows not counting header
        self.wiki_url            = config_obj['wiki_url']
        self.username            = config_obj['username']
        self.password            = config_obj['password']
        self.title_tmpl          = config_obj['title_tmpl']
        self.toc_name            = config_obj['toc_name']
        self.cat_col             = int(config_obj.get('cat_col', 0))
        self.path_to_api         = config_obj.get('path_to_api')

        if self.path_to_api is None:
          self.path_to_api = "/"

        # First get a row count.  Formerly we would pad the row number 
        # (e.g., row_fmt = "{:0" + str(len(str(wiki_sess.row_count))) + "}") 
        # before using it as part of a page title.  Now that page
        # titles are fully templated, we don't use row numbers for
        # that anymore, but we obtain the count anyway because it will
        # probably come in handy for something else eventually.
        csv_fh = open(csv_file, 'rb')
        self.csv_input = csv.reader(csv_fh,
                                    delimiter=config_obj.get('delimiter', ','),
                                    quotechar=config_obj.get('quotechar', '"'))
        self.row_count = -1
        for row in self.csv_input: 
            self.row_count += 1
        csv_fh.close()

        # Reset the reader for our callers.
        csv_fh = open(csv_file, 'rb')
        self.csv_input = csv.reader(csv_fh,
                                    delimiter=config_obj.get('delimiter', ','),
                                    quotechar=config_obj.get('quotechar', '"'))

        # Connect to the site.
        try:
            self.site_conn = Site(self.wiki_url.split("://"), path=self.path_to_api)
        except requests.exceptions.HTTPError as err: 
            sys.stderr.write("ERROR: failed to connect to wiki URL '%s'\n" % self.wiki_url)
            sys.stderr.write("       Error details:\n")
            sys.stderr.write("       ('%s')\n" % err)
            sys.exit(1)
    
        try:
            self.site_conn.login(self.username, self.password)
        except errors.LoginError as err:
            sys.stderr.write("ERROR: Unable to log in to wiki; "
                             "check that username and password are correct.\n")
            sys.stderr.write("       Error details:\n")
            sys.stderr.write("       ('%s')\n" % err)
            sys.exit(1)

    def wiki_escape(self, s):
        """Return a wiki-escaped version of STRING."""
        # There was a page with a title like
        # "Entry_72_Foo_Bar_]Baz[." and the API rejected
        # that, so I guess we have to do some escaping.  
        # Later some fields with "#N/A" failed (as text in a page, not
        # as page titles).  Below we carelessly lump all of these
        # cases together and use "-" as the go-to replacement.
        return s.replace("]", "-").replace("[", "-").replace("#", "-").replace("/", "-")

# TODO: create_pages() and delete_pages() are MediaWiki-specific right
# now -- they just drive the 'mwclient' calls directly.  If we wanted
# to support other wiki types, the way to do that would be to abstract
# that stuff into new methods in WikiSession and select conditional
# branches based on a new config parameter 'wiki_type'.

def create_pages(wiki_sess):
    """Create one wiki page for each row in the CSV file corresponding
    to WikiSession WIKI_SESS.  The CSV must have a header row and at
    least one row of content."""

    toc_text = ""
    categories = []
    std_edit_msg = "Page generated by csv2wiki "               \
                   + "(https://github.com/OpenTechStrategies/" \
                   + "ots-tools/blob/master/csv2wiki)."

    def massage_string(s):
      """Convert non-ASCII string S to nearest lower ASCII equivalent."""
      # TODO: This is really a todo for the unidecode module
      # (https://pypi.python.org/pypi/Unidecode), not us.  The
      # documentation says to just use the unidecode() function as the
      # entry point.  That's an alias for unidecode_expect_ascii(),
      # which should be the right choice when most of your input is
      # the plain ASCII subset of UTF-8, because it tries to decode
      # using an ASCII assumption and then catches the exception and
      # tries again with non-ascii as the assumption in the rare case
      # that the string is not ASCII.
      #
      # However, I think maybe it has a bug with respect to recent
      # Python versions?  It tries to trap the exception by catching
      # 'UnicodeEncodeError', but the error actually thrown is
      # (reasonably enough) 'UnicodeDecodeError'.  So it just
      # propagates that exception up the stack and never makes it to
      # the unidecode_expect_nonascii() call that would have
      # successfully decoded the string.
      # /usr/local/lib/python2.7/dist-packages/unidecode/__init__.py
      # has the details (on my system, at least).
      #
      # Anyway, the solution is to call unidecode_expect_nonascii()
      # directly and not worry that that's slightly slower overall.
      try:
          s = unicode(s, "utf-8")
      except UnicodeDecodeError:
          # TODO: We could nest more encoding attempts here.
          # E.g., ISO-8859-2, -3, ... -15, KOI8_R, etc.  But
          # information about character encoding frequency is
          # surprisingly hard to come by on the Internet, so it's
          # unclear what order would be best to try them in.
          s = unicode(s, "latin-1")
      return unidecode.unidecode_expect_nonascii(s)

    # read in csv
    is_header = True
    row_num = 0
    for row in wiki_sess.csv_input:
        page = None
        page_name = None
        page_title = None
        page_text = ""
        if is_header:
            # if this is the first row, save headers
            header_array = []
            for cell in row:
                header_array.append(cell)
            is_header = False
        else:
            # Looping over the cells in the row.  Name the sections
            # according to headers.
            cell_num = 0
            cell_text = None
            for cell in row:
                if cell_num == 0:
                    # For this new line, generate a mediawiki page
                    #
                    # Splice in any requested columns.  We prepend the
                    # row_num as the first element in the tuple, which
                    # brings two benefits.  First, the user get access
                    # to the row number, via the special code "{0}" in
                    # title_tmpl in the config file.  Second, all the
                    # other columns, which the user is likely to think
                    # of using 1-based indexing not 0-based, are now
                    # 1-based, because the row number is in the first
                    # slot (slot [0]) of the tuple. 
                    row_num_fmt = "{0:0" + str(len(str(wiki_sess.row_count))) + "d}"
                    row_num_str = row_num_fmt.format(row_num)
                    page_name = wiki_sess.title_tmpl.format(*([row_num_str,] + row))
                    # The input is UTF-8, but for wiki page names we
                    # want to stick to plain old lower ASCII.
                    # 
                    # TODO: This is really a todo for the unidecode
                    # module (https://pypi.python.org/pypi/Unidecode),
                    # not us.  The documentation says to just use the
                    # unidecode() function as the entry point.  That's
                    # an alias for unidecode_expect_ascii(), which
                    # should be the right choice when most of your
                    # input is the plain ASCII subset of UTF-8,
                    # because it tries to decode using an ASCII
                    # assumption and then catches the exception and
                    # tries again with non-ascii as the assumption in
                    # the rare case that the string is not ASCII.
                    # However, I think maybe it has a bug with respect
                    # to recent Python versions?  It tries to trap the
                    # exception by catching 'UnicodeEncodeError',
                    # but the error actually thrown is (reasonably
                    # enough) 'UnicodeDecodeError'.  So it just
                    # propagates that exception up the stack and never
                    # makes it to the unidecode_expect_nonascii() call
                    # that would have successfully decoded the string.
                    # /usr/local/lib/python2.7/dist-packages/unidecode/__init__.py
                    # has the details (on my system, at least).
                    #
                    # Anyway, the solution for us is to just call
                    # unidecode_expect_nonascii() directly and not
                    # worry that that's slightly slower overall.
                    page_name = massage_string(page_name)
                    page_name = page_name.replace(" ", "_")
                    page_name = wiki_sess.wiki_escape(page_name)
                    page = wiki_sess.site_conn.pages[page_name]
                    # Set the contents of each cell to their own section.
                if cell is not "":
                    # A section can only be created with some text
                    # 
                    cell_text = cell
                    if cell_num + 1 == wiki_sess.cat_col:
                        cell_esc = massage_string(wiki_sess.wiki_escape(cell))
                        cell_text = '[[:Category:' + cell_esc + '|' + cell_esc + ']]\n'
                        cell_text += '[[Category:' + cell_esc + ']]'
                        if cell_esc not in categories:
                            categories.append(cell_esc)
                    page_text += "== " + header_array[cell_num] + " =="
                    page_text += "\n\n"
                    page_text += cell_text
                    page_text += "\n\n"
                cell_num += 1
            try:
                page.save(page_text, std_edit_msg)
            except errors.APIError as err:
                sys.stderr.write("ERROR: unable to write page:\n" + "'%s'" % err)
                sys.exit(1)
            page_title = page_name
            toc_text += '* [[' + page_name + ']]\n'
            if row_num > 0:
                print("CREATED PAGE: \"" + page_title + "\"")
        row_num += 1
        
    # create the TOC page.
    toc_page = wiki_sess.site_conn.pages[wiki_sess.toc_name]
    toc_page.save(toc_text, std_edit_msg)
    
    # generate the category pages
    if wiki_sess.cat_col:
        for category in categories:
            page = wiki_sess.site_conn.pages['Category:' + category]
            try:
                page.save("", std_edit_msg)
            except errors.APIError as err:
                sys.stderr.write(
                  "ERROR: unable to create category page '%s':\n" % category)
                sys.stderr.write("       '%s'\n" % err)
                sys.exit(1)
            print("CREATED CATEGORY: \"" + category + "\"")

    return

def delete_pages(wiki_sess, pat):
    """Delete wiki pages in WIKI_SESS's wiki whose names match pattern PAT."""
    search_result = wiki_sess.site_conn.search(pat)
    for result in search_result:
        # get as a page
        page = wiki_sess.site_conn.pages[result['title']]
        # delete with extreme prejudice
        try:
            page.delete()
        except errors.APIError as err:
            sys.stderr.write("ERROR: unable to delete page:\n      '%s'" % err)
            sys.exit(1)
        print("DELETED PAGE: \"" + result['title'] + "\"")

    return

def usage(errout=False):
    """Print a message explaining how to use this script.
    Print to stdout, unless ERROUT, in which case print to stderr."""
    out = sys.stderr if errout else sys.stdout
    out.write(__doc__)

def parse_config_file(config_file):
    """Return a dictionary mapping fields in CONFIG_FILE to their values."""
    # Return a newly-created dictionary, rather than the ConfigParser
    # object itself, because we want to avoid callers having to pass
    # the 'default' section name every time they access a field.
    config_obj = {}
    config = ConfigParser.ConfigParser()
    parsed_files = config.read(config_file)
    # ConfigParser.read() returns the number of files read, and if
    # some of them aren't present, it doesn't raise any exceptions
    # about that, it just moves on to try the next one in the list. 
    # (Yes, one could pass it a list of files instead of a single
    # filename.)  Because of this unusual behavior, explained more in
    # https://docs.python.org/2/library/configparser.html, we can't
    # count on an exception being raised and therefore must manually
    # check the returned list instead.
    if len(parsed_files) == 0:
        raise IOError("failed to read config file '%s'" % config_file)
    elif parsed_files[0] != config_file:
        raise IOError("parsed unexpected config file instead of '%s'" % config_file)
    # We have successfully read the config file, so parse it.
    for option in config.options('default'):
        config_obj[option] = config.get('default', option)
    return config_obj

def main():
    """
    By default, creates wiki pages from a supplied CSV.  Optionally,
    deletes those pages instead.

    """
    try:
        opts, args = getopt.getopt(sys.argv[1:], 
                                   'h?d:c:',
                                   ("help",
                                    "usage",
                                    "delete-matching=",
                                    "config="))
    except getopt.GetoptError as err:
        sys.stderr.write("ERROR: '%s'\n" % err)
        usage(errout=True)
        sys.exit(2)

    csv_file = None
    config_file = None
    wiki_sess = None
    delete_matching = None
    bad_opt_seen = False
    for o, a in opts:
        if o in ("-h", "-?", "--help", "--usage"):
            usage()
            sys.exit(0)
        elif o in ("-d", "--delete-matching"):
            delete_matching = a
        elif o in ("-c", "--config"):
            config_file = a
        else:
            sys.stderr.write("ERROR: unrecognized option '%s'\n" % o)
            bad_opt_seen = True

    if bad_opt_seen:
        sys.exit(2)

    if config_file is None:
        sys.stderr.write("ERROR: missing config file; use -c to supply it\n")
        usage(errout=True)
        sys.exit(2)
    else:
        config_settings = parse_config_file(config_file)

    if len(args) == 1:
        wiki_sess = WikiSession(args[0], config_settings)
    elif len(args) < 1:
        sys.stderr.write("ERROR: missing CSV file argument\n")
        usage(errout=True)
        sys.exit(2)
    else:
        sys.stderr.write("ERROR: too many arguments\n")
        usage(errout=True)
        sys.exit(2)

    if delete_matching is not None:
        try:
            delete_pages(wiki_sess, delete_matching)
        except IndexError as err:
            sys.stderr.write("ERROR: '%s'\n" % err)
            usage(errout=True)
            sys.exit(1)
    else:
        try:
            create_pages(wiki_sess)
        except IndexError as err:
            sys.stderr.write("ERROR: '%s'\n" % err)
            usage(errout=True)
            sys.exit(1)


if __name__ == '__main__':
    main()
